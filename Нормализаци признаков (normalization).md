parent: [[Приведение признаков к одному масштабу]]

tags: #normalization #ml

Чаще всего под норма­ лизацией понимается изменение масштаба признаков на диапазон $[0, 1]$, которое представляет собой частный случай масштабирования по минимаксу (min-max scaling). Для нормализации данных мы можем просто применить масштабирование по минимаксу к каждому столбцу признака, где новое значение $x_{norm}^{(i)}$ образца $х^{(i)}$ может быть вычислено как:
$$x_{norm}^{(i)}=\frac{x^{(i)}-x_{min}}{x_{max}-x_{min}}$$
Здесь $x^{(i)}$ - индивидуальный образец, $x_{min}$ - минимальное значение в столбце признака, а $x_{max}$ - максимальное в столбце признака.

Процедура масштабирования по минимаксу реализована в библиотеке scikit-learn и может использоваться следующим образом:

```python
from sklearn.preprocessing import MinMaxScaler 

mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
```

Не смотря на то, что нормализация часто используемый прием, наиболее практичной для ML чаще оказывается [[Стандартизация признаков (standartization)|стандартизация]], особенно для алгоритмов оптимизации на основе градиентного спуска.