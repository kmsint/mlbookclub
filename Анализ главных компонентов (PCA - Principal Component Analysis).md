parent: [[Сжатие данных с помощью понижения размерности]]

tags: #ml #pca 

Анализ главных компонентов помогает распознавать шаблоны в данных, основываясь на корреляции между признаками. Выражаясь кратко, анализ РСА нацелен на обнаружение направлений максимальной [[Дисперсия (Variance)|дисперсии]] в данных высокой размерности и проецирование их в новое подпространство с равным или меньшим числом измерений.

[[Метод главных компонент на простом примере]] 

Ортогональные оси (главные компоненты) нового подпространства можно интерпретировать как направления максимальной дисперсии при условии ограничения, что новые оси признаков ортогональны друг к другу

![[Screenshot 2023-12-16 at 14.23.34.png]]

Здесь $x_1$ и $x_2$ - исходные оси признаков, а $PC 1$ и $PC 2$ - главные компоненты.

Когда применяется PCA для понижения размерности - строится матрица трансформации $W$ размерностью ($d*k$), позволяющая отобразить вектор $х$ с признаками обучающего образца на новое $k$-мерное подпространство признаков, которое имеет меньше измерений, чем исходное $d$-мерное пространство признаков.

В результате трансформации исходных d-мерных данных в новое $k$-мерное подпространство (обычно $k$ << $d$) первый главный компонент будет располагать наибольшей дисперсией из возможных. Все последующие главные компоненты будут иметь наибольшую дисперсию при условии, что они не взаимосвязаны (ортогональны) с другими главными компонентами - даже если входные признаки взаимосвязаны, то результирующие главные компоненты будут взаимно ортогональными (не взаимосвязанными).

Обратите внимание, что направления РСА крайне чувствительны к масштабированию данных, поэтому перед проведением анализа РСА нам придется стандартизировать признаки, если они были измерены с разными масштабами и мы хотим придать равную важность всем признакам.

[[Основные шаги при анализе главных компонентов]]




