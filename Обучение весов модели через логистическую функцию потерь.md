parent: [[Обзор классификаторов на основе ML с использованием scikit-learn]]

tags: #ml #mlbookclub #reading #scikitlearn #logistic_regression 

[[Объяснение логистической регрессии|Вот здесь]] более удачное объяснение логистической регрессии, чем в книге Рашки.

Чтобы пояснить, как можно вывести функцию потерь для логистической регрессии, первым делом определим [[Функция правдоподобия|правдоподобие]] (likelihood) $L$, которое мы хотим довести до [[Оценка максимального правдоподобия|максимума]] при построении логистической регрессионной модели с условием, что индивидуальные образцы в наборе данных независимы друг от друга.

$$\mathcal{L}(w,b|x)=p(y|x;w,b)=\prod_{i=1}^np(y^{(i)})|x^{(i)};w, b)=\prod_{i=1}^n(\sigma(z^i))^{y^{(i)}}(1-\sigma(z^{(i)}))^{1-y^{(i)}}$$
На практике проще довести до максимума натуральный логарифм этого правдоподобия, который называется функцией логарифмического правдоподобия (log-likelihood):
$$l(w,b|x)=ln\mathcal{L}(w,b|x)=\sum_{i=1}[y^{(i)}ln(\sigma(z^{(i)}))+(1-y^{(i)})ln(1-\sigma(z^{(i)}))]$$
Во-первых, применение логарифмической функции снижает возможность числовой потери значимости, которая может произойти, если вели­ чины правдоподобия очень малы. Во-вторых, мы можем преобразовать произведение сомножителей в сумму сомножителей, что облегчит получение производной этой функции посредством приема со сложением.

