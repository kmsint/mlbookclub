parent: [[Обзор классификаторов на основе ML с использованием scikit-learn]]

tags: #ml #mlbookclub #reading #scikitlearn

**Переобучение** (overfitting) - распространенная проблема в ML, когда модель хорошо работает на обучающих данных, но плохо обобщается на не встречавшиеся ранее (тестовые) данные. Если модель страдает от переобучения, то мы также говорим, что она имеет высокую дисперсию, которая может быть связана с наличием слишком большого числа параметров, приводящих к получению чересчур сложной модели для лежащих в основе данных. Подобным же образом модель может также страдать от недообучения (underfitting), или высокого смещения, которое означает, что модель недостаточно сложна для того, чтобы хорошо выявлять структуру в обучающих данных, из-за чего она обладает низкой эффективностью на не встречавшихся ранее данных.

![[Screenshot 2023-12-03 at 01.37.09.png]]

При описании эффективности модели исследователи часто исполь­зуют термины "смещение" и "[[Дисперсия (Variance)|дисперсия]]" или "компромисс между смещением и дисперсией" - т.е. в беседах, книгах или статьях вы можете встретить заявления о том, что модель имеет "высокую дисперсию" или "высокое смещение". Что же это означает? В целом мы можем говорить, что "высокая дисперсия" пропорциональна пере­обучению, а "высокое смещение" - недообучению.

В контексте моделей ML дисперсия измеряет постоянство (либо изменчивость) прогноза модели для классификации отдельного образца при многократном обучении модели, например, на разных подмножествах обучающего набора данных. Мы можем сказать, что модель чувствительна к случайности обучающих данных. Напротив, смещение измеряет, насколько далеко прогнозы находятся от корректных значений в целом при многократном обучении модели на разных обучающих наборах данных; смещение представляет собой меру систематической ошибки, которая не является результатом случайности.

Один из способов поиска хорошего компромисса между смещением и дисперсией предусматривает настройку сложности модели посредством регуляризации.

Регуляризация - еще одна причина важности масштабирования признаков, такого как [[Стандартизация признаков (standartization)|стандартизация]]. Для надлежащей работы ре­гуляризации нужно обеспечить наличие у всех признаков соизмеримых масштабов.