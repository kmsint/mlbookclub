parent: [[Обучение простых ML-алгоритмов для классификации]]

tags: #reading #mlbookclub #ml 

Компромиссом между градиентным спуском, когда для каждого шага алгоритма требуется посчитать ошибку на всей обучающей выборке и SGD, когда ошибка считается по каждому образцу и сразу обновляются параметры модели, является batch-mini gradient descent. Когда обучение происходит на подвыборках определенного размера из обучающего датасета.

Такой алгоритм достигает сходимости быстрее, чем пакетный градиентный спуск, и можно повысить эффективность SGD за счет замены цикла for на операции линейной алгебры.