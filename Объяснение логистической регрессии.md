parent: [[Обучение весов модели через логистическую функцию потерь]]

tags: #ml #mlbookclub #reading #scikitlearn #logistic_regression 

За основу взята [статья с Хабра](https://habr.com/ru/companies/io/articles/265007/).

Разделение данных для линейных моделей происходит с помощью линейной границы - дискриминатора. Пример такой границы на рисунке:

![[Pasted image 20231201000257.png]]

Задача логистической регрессии предсказать вероятность принадлежности объекта к тому или иному классу. А решение о принадлежности к определенному классу можно принять исходя из того больше эта вероятность 0.5 или меньше. То есть, если модель выдаст значение, например, 0.8, то можно будет сказать, что объект принадлежит к классу 1 с вероятностью 80%, и тогда легко понять, что вероятность принадлежности объекта к классу 2 будет 0.2 (20%), т.к. сумма вероятностей обоих исходов (принадлежности к 1-му классу и принадлежности ко 2-му классу) равна 1. А $$1 - 0.8 = 0.2$$Вероятность всегда лежит в диапазоне $[0;1]$, а значение, которое получается на входе линейной модели в диапазоне $(-\infty;+\infty)$:
$$z=w_1x_1+w_2x_2+...+w_mx_m+b$$
Это линейная комбинация весов и входов (признаков, ассоциированных с обучающими образцами). Если $z^{(i)}>0$, тогда объект с признаками $(x_1, x_2, ..., x_m)$ принадлежит классу 1, а если  $z^{(i)}<0$ - классу 2. Если же $z=0$, тогда мы не можем причислить объект к одному из классов. Чем дальше значение $z^{(i)}$ от нуля в сторону положительных значений то есть лежащих в диапазоне $(0;+\infty)$, тем выше вероятность принадлежности объекта к классу 1 и эта вероятность лежит в диапазоне $(0.5;1]$ и, соответственно, чем дальше значение $z^{(i)}$ от нуля в сторону отрицательных значений, то есть лежащих в диапазоне $(-\infty;0)$, тем меньше вероятность принадлежности объекта к классу 1 и вероятность лежит в диапазоне $[0;0.5)$.

Соответственно, нужно найти способ отображения диапазона $(-\infty;+\infty)$ в диапазон $[0;1]$ 

Поступают следующим образом. Берут не вероятность принадлежности объекта к какому-то классу, а отношение шансов. Значение такого отношения лежит в диапазоне $[0;+\infty)$, а затем логарифмируют по основанию $e$, отображая отношение шансов в требуемый диапазон $(-\infty;+\infty)$.

Отношение шансов - это отношение вероятности исхода одного события к вероятности исхода остальных событий. В нашем случае исходов может быть 2 (принадлежность к классу 1 и принадлежность объекта к классу 2). Вероятность первого исхода $p$, а второго $1-p$, тогда отношение шансов ($odds$) - это:
$$odds=\frac{p}{1-p}$$
Вероятность и отношение шансов содержат одинаковую информацию. Но при этом вероятность находится в диапазоне $[0;1]$, а отношение шансов ($odds$) уже в диапазоне $[0;+\infty)$, как было сказано ранее. Если теперь прологарифмировать отношение шансов, то получим функцию логистического отклика:
$$ln(odds)=ln\frac{p}{1-p}=w_1x_1+w_2x_2+...+w_mx_m+b=w^Tx+b$$
То есть, зная значение функции $z$ можно выразить шансы ($odds$):
$$odds=e^{w^Tx+b}$$
Чтобы перейти от шансов к вероятности, нужно вероятность выразить через шансы:
$$odds=\frac{p}{1-p}$$
$$odds(1-p)=p$$
$$odds-p\cdot odds=p$$
$$odds=p+p\cdot odds$$
$$odds=p(1+odds)$$
$$p=\frac{odds}{(1+odds)}$$
Ну, а т.к. ранее мы получили, что $odds=e^{w^Tx+b}$, вероятность можно выразить как:
$$p=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}$$
Ну, или:
$$p=\frac{e^z}{1+e^z}$$
Если теперь еще и числитель и знаменатель разделить на $e^z$ - получится:
$$p=\frac{1}{1+e^{-z}}=\sigma(z)$$

Вот так получается сигмоид-функция или, по-другому, функция логистического отклика, превращающая "отдаление" объекта от разделяющей поверхности в вероятность принадлежности этого объекта к одному из классов.

Теперь нужно разобраться как подбирать параметры модели через максимизацию [[Функция правдоподобия|правдоподобия]]. 

Когда мы говорили о том, что вероятность принадлежности объекта к классу 1 - это $p$, а вероятность принадлежности объекта к классу 2 - это $1-p$, мы пользовались так называемой обобщенной схемой Бернулли, потому что схема Бернулли - это последовательность независимых испытаний, в каждом из которых возможны лишь 2 исхода - "успех" и "неудача", при этом успех в каждом испытании происходит с одной и той же вероятностью $p$, а неудача - с вероятностью $q=1-p$ 

Плотность распределения Бернулли задается следующей формулой:
$$f(k;p) = \begin{cases} p, & \text{если } k = 1, \\ q = 1 - p, & \text{если } k = 0. \end{cases}$$
Здесь $p$ - вероятность успеха, а $q$ - вероятность неудачи.

Формула правдоподобия для распределения Бернулли выглядит следующим образом:

$$\mathcal{L}(p) =\prod_{i=1}^np^{y^{(i)}}(1-p)^{1-y^{(i)}}$$
где:
- $p$ - вероятность успеха,
- $k$ - наблюдаемое значение (0 или 1),
- $n$ - количество образцов.

Ранее мы выразили вероятность через сигмоид-функцию от входа, тогда правдоподобие можно записать следующим образом:
$$\mathcal{L}(w,b|x)=\prod_{i=1}^n(\sigma(z^{(i)}))^{y^{(i)}}(1-\sigma(z^{(i)}))^{1-y^{(i)}}$$
И теперь, чтобы подобрать веса входа - нужно максимизировать функцию правдоподобия. Нужно вычислить частные производные по всем весам и двигаться в сторону градиента (градиентный подъем). 

На практике работают не с самой функцией правдоподобия, а с ее логарифмом, потому что логарифм произведения является суммой логарифмов множителей, а сумма легче дифференцируется. Также меняют знак получившегося выражения, потому что на практике принято минимизировать какую-либо функцию для поиска оптимума, а не максимизировать.

Если опустить все математические преобразования по вычислению производных, то в конечном итоге придем к формуле обновления весов:

