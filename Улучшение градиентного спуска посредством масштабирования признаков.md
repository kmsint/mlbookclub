parent: [[Обучение простых ML-алгоритмов для классификации]]

tags: #reading #mlbookclub #ml 

Многие алгоритмы машинного обучения требуют определенного масштабирования признаков для оптимальной эффективности. Один из подходов к масштабированию - это стандартизация, то есть придание данным характеристик стандартного нормального распределения (нулевое среднее и единичную дисперсию). Такая процедура нормализации содействует более быстрому схождению обучения с градиентным спуском, однако, она не делает исходный набор данных нормально распре­деленным. Стандартизация так смещает среднее каждого признака, что оно центрируется возле нуля, и каждый признак имеет стандартное отклонение 1 (единичную дисперсию).

Например, чтобы стандартизировать $j$-й признак, можно просто вычесть выборочное среднее $\mu_j$ из каждого обучающего образца и разделить результат на стандартное отклонение $\sigma_j$:
$$x_j^\prime=\frac{x_j-\mu_j}{\sigma_j}$$
Здесь $x_j$ - вектор, состоящий из значений $j$-го признака всех обучающих образцов $n$, и этот прием стандартизации применяется к каждому признаку $j$ в наборе данных.

Одна из причин, по которой стандартизация помогает алгоритму ML, это уменьшение пути, который нужно пройти оптимизатору, чтобы найти глобальный минимум

![[Screenshot 2023-11-23 at 20.48.02.png]]

Для стандартизации используются, в том числе, методы NumPy - mean и std

```python
import numpy as np

# standardize features
X_std = np.copy(X)
X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()
X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()
```

