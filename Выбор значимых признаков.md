parent: [[Построение хороших обучающих наборов - предварительная обработка данных]]

tags: #ml #mlbookclub #reading 

Если мы замечаем, что модель работает гораздо лучше на обучающем наборе данных, чем на испытательном, то такое наблюдение является явным сигналом [[Переобучение (Overfiting)|переобучения]].

Переобучение означает, что модель слишком сильно подгоняется к параметрам касаемо отдельных наблюдений в обучающем наборе данных, но плохо обобщается на новые данные. Также говорят, что модель имеет высокую [[Дисперсия (Variance)|дисперсию]]. Причина переобучения состоит в том, что модель является слишком сложной для имеющихся обучающих данных.

Общепринятые решения, направленные на сокращение ошибки обобщения:

- Накопление большего количества обучающих данных
- Введение штрафа за сложность через [[Регуляризация L1 и L2 как штрафы за сложность модели|регуляризацию]]
- Выбор более простой модели с меньшим числом параметров
- Понижение размерности данных

1. [[Регуляризация L1 и L2 как штрафы за сложность модели]]
2. [[Геометрическая интерпретация регуляризации L2]]
3. [[Разреженные решения с регуляризацией L1]]
4. [[Алгоритмы последовательного выбора признаков]]

