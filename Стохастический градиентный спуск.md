parent: [[Обучение простых ML-алгоритмов для классификации]]

tags: #reading #mlbookclub #ml 

При обучении Adaline мы использовали градиентный спуск, который завязан на вычисление градиента для всего датасета (пакетный градиентный спуск - batch gradient descent) и это не проблема только когда датасеты не очень большие. Но в реальной жизни обычно датасеты могут быть большими и очень большими и тогда вычисление градиентов по всему набору данных будет требовать значительных ресурсов, ведь на каждом шаге по направлению к глобальному минимуму придется заново оценить полный обучающий набор.

Альтернативой пакетному градиентному спуску является стохастический градиентный спуск (stochastic gradient descent - SGD), называемый еще итеративным или динамическим градиентным спуском. Суть его в том, что вместо обновления весов на основе суммы накопленных ошибок по всем образцам $x^{(i)}$:
$$\Delta w_j=\frac{2\eta}{n}\sum_i(y^{(i)}-\sigma(z^{(i)}))x_j^{(i)}$$
веса обновляются постепенно для каждого обучающего образца:
$$\Delta w_j=\eta(y^{(i)}-\sigma(z^{(i)}))x_j^{(i)},\qquad \Delta b=\eta(y^{(i)}-\sigma(z^{(i)}))$$
SGD обычно быстрее достигает сходимости алгоритма из-за более частого обновления весов. Из-за того, что каждый градиент вычисляется на основе одного образца, поверхность ошибок оказывается зашумленнее, чем при градиентном спуске. Это также дает преимущество в том, что SGD позволяет с большей легкостью избежать локальных минимумов для нелинейных функций издержек.

Чтобы получить удовлетворительные результаты посредством SGD, важно подавать ему обучающие данные в случайном порядке, а для избегания циклов необходимо перемешивать набор для каждой эпохи обучения.

Еще одно преимущество SGD в том, что алгоритм может дообучаться "на лету" по мере поступления новых обучающих данных, что позволяет системе адаптироваться к меняющимся условиям среды. А в случае ограниченного пространства для хранения данных, они могут выбрасываться сразу после обновления модели.