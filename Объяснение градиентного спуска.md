parent: [[Минимизация функции потерь с помощью градиентного спуска]]

tags: #reading #mlbookclub #ml 

У нас есть какая-то модель. Пусть для примера это будет линейная регрессия с одним признаком и смещением. Допустим, нужно предсказать зависимость веса человека от его роста. Здесь рост будет предиктором или признаком, а вес - зависящей переменной или таргетом.

Линейную модель можно описать формулой:
$$y=kx+b$$
Здесь $k$ - это весовой коэффициент при признаке $x$, а $b$ - смещение, то есть можно переписать формулу модели в привычных обозначениях как:
$$z=w_1x+b$$
Но так как весов всего 1, то можно и еще проще:
$$z=wx+b$$
Далее, у нас есть функция потерь, которую нужно минимизировать. Это часто MSE - среднеквадратичная ошибка:
$$L(w, b)=\frac{1}{2n}\sum_{i=1}^n(y^{(i)}-\sigma(z^{(i)}))^2$$
Чтобы подобрать коэффициенты модели ($w$ и $b$) - нужно минимизировать значение этой функции потерь, то есть найти при каких значениях $w$ и $b$ она будет минимальна. 

Из математического анализа известно, что в точках экстремума (минимума, максимума или перегиба функции) производная функции равна нулю. То есть в идеале, нам нужно найти в какой точке функции потерь ее производная будет равна нулю. Но часто именно ноль для сложных функций найти непросто и поэтому ищут не ноль, а какое-то значение, близкое к нулю, удовлетворяющее решению задачи.

Чтобы найти минимум фунции поступают следующим образом. Берут произвольную точку и вычисляют градиент в этой точке. Градиент - это вектор, показывающий направление возрастания функции. Соответственно, чтобы сделать шаг в сторону минимума функции - нужно идти в направлении, противоположном направлению градиента.

Градиент можно представить как сумму частных производных по каждому измерению функции. В нашей задаче, когда есть только один признак и смещение, таких частных производных будет 2 - по весовому коэффициенту $w$ и смещению $b$

В принципе, теперь достаточно информации, чтобы начать обучение модели. Алгоритм такой:

1. Инициализируем нашу модель какими-то случайными весами (то есть берем случайные значения для $w$ и $b$)
2. Берем первую пару признак-метка - и считаем насколько наша модель (с заданным случайным весом и смещением) для данного признака отклоняется от таргета (квадрат отклонения), сохраняем
3. Берем вторую пару признак-метка и снова считаем насколько модель отклоняется от таргета
4. Берем следующую пару и тоже считаем квадрат отклонения
5. Так делаем, пока пары в тренировочном датасете не кончатся
6. После того, как был посчитан квадрат отклонения для всех пар трейна, нужно эти отклонения просуммировать и разделить на количество пар. Тем самым будет найдена та самая среднеквадратичная ошибка (MSE) модели для точки $(w,b)$ 
7. Теперь, зная эту ошибку - нужно вычислить градиент в данной точке функции потерь, чтобы определиться в какую сторону изменять $w$ и $b$. Для этого нужно посчитать частные производные функции потерь в этой точке.
8. Когда посчитана частная производная функции потерь в точке $(w,b)$ для $w$ - можно найти $\Delta w$ - то значение, которое нужно прибавить к текущему значению $w$, чтобы получить новое (обновленное значение $w$ для следующей эпохи обучения). Для этого значение частной производной умножают на скорость обучения и на -1. -1 здесь, как понятно, для того, чтобы изменять значение $w$ в сторону минимизации функции потерь
9. Так же поступают и со смещением, вычисляя $\Delta b$ и суммируя его с предыдущим значением $b$, чтобы обновить перед следующей эпохой обучения
10. Теперь, перед следующей эпохой обучения у нас есть обновленные значения $w$ и $b$ и нужно заново посчитать MSE для этих значений по всем образцам трейна.
11. Затем снова вычислить частные производные и обновить $w$ и $b$
12. Так повторять до тех пор, пока значение MSE не достигнет минимума, то есть значения, приемлемого для решения задачи.
13. Часто алгоритм останавливают, если предыдущее значение MSE отличается от текущего не более, чем на какое-то пороговое значение, которое мы сами установили. В этом случае говорят о том, что алгоритм сошелся и модель считается обученной.