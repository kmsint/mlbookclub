parent: [[Приведение признаков к одному масштабу]]

tags: #standartization #ml 

Суть стандартизации заключается в центрировании столбцов признаков относительно среднего значения 0 со стандартным отклонением 1, чтобы столбцы признаков имели те же параметры, что и нормальное распределение (нулевое среднее и единичная дисперсия), облегчая выяснение весов модели.

Кроме того, стандартизация сохраняет полезную информацию о выбросах и делает алгоритм менее чувствительным к ним, в отличие от [[Нормализаци признаков (normalization)|масштабирования по минимаксу]], которое приводит данные к ограниченному диапазону значений. 

Формула стандартизации:
$$x_{str}^{(i)}=\frac{x^{(i)}-\mu_x}{\sigma_x}$$
Здесь $\mu_x$ - [[Выборочное среднее (Sample mean)|выборочное среднее]] по определенному столбцу признака, а $\sigma_x$ - соответствующее [[Стандартное отклонение (Standard deviation)|стандартное отклонение]].

В качестве примера таблица со сравнением стандартизации и нормализации для простого набора, состоящего из целых чисел от 0 до 5:

![[Screenshot 2023-12-10 at 20.15.26.png]]

В sklearn уже реализован готовый класс для стандартизации `MinMaxScaler`:

```python
from sklearn. preprocessing import StandardScaler 

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)
```

Также важно отметить, что мы подгоняем класс `StandardScaler` только один раз - на обучающих данных - и применяем найденные параметры для трансформирования тестового набора или любой новой точки данных.

